{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOY3/EoIVv0C0bUqjLVSQ7C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# AB TEST"],"metadata":{"id":"mr8Lk8VNvGN9"}},{"cell_type":"markdown","source":["**EX_1**"],"metadata":{"id":"oK8BdhzcvCLB"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MUYIq3FAvBQU","executionInfo":{"status":"ok","timestamp":1728216639181,"user_tz":-180,"elapsed":3194,"user":{"displayName":"Manuel Kizer","userId":"07103369823277029587"}},"outputId":"a2f04329-3fb1-42aa-87b3-531406ed43f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Required sample size per group: 2940.557827324\n"]}],"source":["from statsmodels.stats.power import NormalIndPower\n","from statsmodels.stats.proportion import proportion_effectsize\n","\n","# Parameters\n","baseline_rate = 0.20  # Baseline open rate\n","new_rate = 0.23  # New open rate (after changing the subject line)\n","effect_size = proportion_effectsize(baseline_rate, new_rate)  # Calculate effect size\n","alpha = 0.05  # Significance level\n","power = 0.80  # Desired power (80%)\n","\n","# Calculate required sample size\n","power_analysis = NormalIndPower()\n","sample_size = power_analysis.solve_power(effect_size, power=power, alpha=alpha, ratio=1)\n","print(\"Required sample size per group:\", sample_size)\n"]},{"cell_type":"markdown","source":["This means you would need approximately 3673 participants in each group (A and B) to ensure that your A/B test is properly powered"],"metadata":{"id":"uqfnZhBCwsYI"}},{"cell_type":"markdown","source":["**EX_2**"],"metadata":{"id":"PPhqX_-gwvN6"}},{"cell_type":"code","source":["from statsmodels.stats.power import NormalIndPower\n","\n","# Initialize power analysis object\n","power_analysis = NormalIndPower()\n","\n","# Parameters\n","alpha = 0.05  # Significance level\n","power = 0.80  # Power level\n","\n","# Effect sizes\n","effect_sizes = [0.2, 0.4, 0.5]\n","\n","# Calculate sample sizes for each effect size\n","for effect_size in effect_sizes:\n","    sample_size = power_analysis.solve_power(effect_size, power=power, alpha=alpha, ratio=1)\n","    print(f\"Required sample size for effect size {effect_size}: {sample_size:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"McdR9Aapww-x","executionInfo":{"status":"ok","timestamp":1728217105867,"user_tz":-180,"elapsed":343,"user":{"displayName":"Manuel Kizer","userId":"07103369823277029587"}},"outputId":"e3eb37ac-d26b-40d2-84c6-3624ea66c827"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Required sample size for effect size 0.2: 392.44\n","Required sample size for effect size 0.4: 98.11\n","Required sample size for effect size 0.5: 62.79\n"]}]},{"cell_type":"markdown","source":["A small effect size means that the difference between the two groups (A and B) is relatively small. As a result, we need a larger sample size to detect this small difference with high confidence"],"metadata":{"id":"S0AXrb3Ix-8b"}},{"cell_type":"markdown","source":["**EX_3**"],"metadata":{"id":"1Qh_Xtt6x3RW"}},{"cell_type":"code","source":["from statsmodels.stats.power import NormalIndPower\n","\n","# Initialize power analysis object\n","power_analysis = NormalIndPower()\n","\n","# Parameters\n","effect_size = 0.2  # Small effect size\n","alpha = 0.05  # Significance level\n","\n","# Power levels\n","power_levels = [0.7, 0.8, 0.9]\n","\n","# Calculate sample sizes for each power level\n","for power in power_levels:\n","    sample_size = power_analysis.solve_power(effect_size, power=power, alpha=alpha, ratio=1)\n","    print(f\"Required sample size for power level {power}: {sample_size:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ZR0JuHjx01K","executionInfo":{"status":"ok","timestamp":1728217306853,"user_tz":-180,"elapsed":320,"user":{"displayName":"Manuel Kizer","userId":"07103369823277029587"}},"outputId":"868f4941-3232-48b5-a58a-8ac21c7dc0e2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Required sample size for power level 0.7: 308.60\n","Required sample size for power level 0.8: 392.44\n","Required sample size for power level 0.9: 525.37\n"]}]},{"cell_type":"markdown","source":["**EX_4**"],"metadata":{"id":"ZTaq2lDqyDzW"}},{"cell_type":"markdown","source":["Bonferroni Correction: Divide the significance level by the number of tests (weeks).\n","\n","Pocock Boundary: Adjusts the significance level based on the number of analyses, creating a more flexible boundary.\n","\n","Oâ€™Brien-Fleming Boundary: Starts with more conservative significance levels early in the test and gradually becomes more lenient."],"metadata":{"id":"ll_-IlrA02QU"}},{"cell_type":"markdown","source":["Bonferroni Correction Method"],"metadata":{"id":"Qe_zPfIq48Hd"}},{"source":["# Pseudocode for sequential testing\n","import scipy.stats as stats\n","\n","# Initialize variables\n","weeks = 6\n","alpha = 0.05\n","adjusted_alpha = alpha / weeks  # Bonferroni correction\n","\n","# Monitor weekly\n","for week in range(1, weeks + 1):\n","    # Assume 'calculate_p_value' is a function that returns the p-value for the current week's data\n","    # You need to define the function 'calculate_p_value'\n","    def calculate_p_value(week):\n","        # Replace this with your actual p-value calculation logic\n","        # This example just returns a random p-value\n","        return stats.uniform.rvs()\n","\n","    p_value = calculate_p_value(week)\n","\n","    print(f\"Week {week}, p-value: {p_value}\")\n","\n","    # Check if the p-value is below the adjusted threshold\n","    if p_value < adjusted_alpha:\n","        print(f\"Significant result at week {week}. Stop the test.\")\n","        break\n","    else:\n","        print(\"Continue testing.\")"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fVZJzbp8yoid","executionInfo":{"status":"ok","timestamp":1728217518384,"user_tz":-180,"elapsed":318,"user":{"displayName":"Manuel Kizer","userId":"07103369823277029587"}},"outputId":"0f580c5b-f730-4988-c9e0-5999113a87d5"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Week 1, p-value: 0.8376318262999883\n","Continue testing.\n","Week 2, p-value: 0.36077161203876695\n","Continue testing.\n","Week 3, p-value: 0.07795096872984342\n","Continue testing.\n","Week 4, p-value: 0.5634853453373877\n","Continue testing.\n","Week 5, p-value: 0.5618426434212065\n","Continue testing.\n","Week 6, p-value: 0.42466524373754966\n","Continue testing.\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from statsmodels.stats.proportion import proportions_ztest\n","\n","# Simulate weekly data (you would replace these with actual numbers)\n","# Data format: (conversions in A, total in A, conversions in B, total in B)\n","weekly_data = {\n","    1: [120, 1000, 130, 1000],\n","    2: [150, 1000, 160, 1000],\n","    3: [180, 1000, 210, 1000],\n","    4: [210, 1000, 240, 1000],\n","    5: [230, 1000, 260, 1000],\n","    6: [250, 1000, 280, 1000]\n","}\n","\n","def calculate_p_value(week):\n","    data = weekly_data[week]\n","    conversions_A, total_A, conversions_B, total_B = data\n","\n","    # Perform Z-test for proportions\n","    counts = np.array([conversions_A, conversions_B])  # successes\n","    nobs = np.array([total_A, total_B])  # total participants\n","    stat, p_value = proportions_ztest(counts, nobs)\n","\n","    return p_value\n","\n","# Sequential Testing Code (as before)\n","weeks = 6\n","alpha = 0.05\n","adjusted_alpha = alpha / weeks  # Bonferroni correction\n","\n","for week in range(1, weeks + 1):\n","    p_value = calculate_p_value(week)\n","    print(f\"Week {week}, p-value: {p_value:.4f}\")\n","\n","    if p_value < adjusted_alpha:\n","        print(f\"Significant result at week {week}. Stop the test.\")\n","        break\n","    else:\n","        print(\"Continue testing.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yiw4aIP8yxMN","executionInfo":{"status":"ok","timestamp":1728217554735,"user_tz":-180,"elapsed":319,"user":{"displayName":"Manuel Kizer","userId":"07103369823277029587"}},"outputId":"91953dd8-f8f7-4af9-87fb-51c605c3229c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Week 1, p-value: 0.4990\n","Continue testing.\n","Week 2, p-value: 0.5367\n","Continue testing.\n","Week 3, p-value: 0.0904\n","Continue testing.\n","Week 4, p-value: 0.1082\n","Continue testing.\n","Week 5, p-value: 0.1188\n","Continue testing.\n","Week 6, p-value: 0.1285\n","Continue testing.\n"]}]},{"cell_type":"markdown","source":["Oâ€™Brien-Fleming Boundary"],"metadata":{"id":"2VNzgFtf5MVf"}},{"cell_type":"code","source":["import numpy as np\n","from statsmodels.stats.proportion import proportions_ztest\n","\n","# Simulated weekly data: (conversions in A, total in A, conversions in B, total in B)\n","weekly_data = {\n","    1: [120, 1000, 130, 1000],\n","    2: [150, 1000, 160, 1000],\n","    3: [180, 1000, 210, 1000],\n","    4: [210, 1000, 240, 1000],\n","    5: [230, 1000, 260, 1000],\n","    6: [250, 1000, 280, 1000]\n","}\n","\n","def calculate_p_value(week):\n","    data = weekly_data[week]\n","    conversions_A, total_A, conversions_B, total_B = data\n","\n","    # Perform Z-test for proportions\n","    counts = np.array([conversions_A, conversions_B])  # successes\n","    nobs = np.array([total_A, total_B])  # total participants\n","    stat, p_value = proportions_ztest(counts, nobs)\n","\n","    return p_value\n","\n","# --- O'Brien-Fleming Boundary Method ---\n","print(\"---- O'Brien-Fleming Boundary Method ----\")\n","\n","# O'Brien-Fleming boundaries (these are approximate critical values for 6 looks)\n","obrien_fleming_boundaries = [0.0005, 0.0032, 0.0085, 0.0169, 0.0283, 0.0437]\n","\n","for week in range(1, 7):\n","    p_value = calculate_p_value(week)\n","    boundary = obrien_fleming_boundaries[week - 1]  # Select the boundary for the current week\n","\n","    print(f\"Week {week}, p-value: {p_value:.4f}, boundary: {boundary:.4f}\")\n","\n","    if p_value < boundary:\n","        print(f\"Significant result at week {week}. Stop the test.\")\n","        break\n","    else:\n","        print(\"Continue testing.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sncxu8QC44m1","executionInfo":{"status":"ok","timestamp":1728219228305,"user_tz":-180,"elapsed":301,"user":{"displayName":"Manuel Kizer","userId":"07103369823277029587"}},"outputId":"10d9c769-8d82-4e91-8bba-b72ac2274fdf"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["---- O'Brien-Fleming Boundary Method ----\n","Week 1, p-value: 0.4990, boundary: 0.0005\n","Continue testing.\n","Week 2, p-value: 0.5367, boundary: 0.0032\n","Continue testing.\n","Week 3, p-value: 0.0904, boundary: 0.0085\n","Continue testing.\n","Week 4, p-value: 0.1082, boundary: 0.0169\n","Continue testing.\n","Week 5, p-value: 0.1188, boundary: 0.0283\n","Continue testing.\n","Week 6, p-value: 0.1285, boundary: 0.0437\n","Continue testing.\n"]}]},{"cell_type":"markdown","source":["Pocock Boundary Method"],"metadata":{"id":"W8vLkAnR5Zv9"}},{"cell_type":"code","source":["import numpy as np\n","from statsmodels.stats.proportion import proportions_ztest\n","from scipy.stats import norm\n","\n","# Simulated weekly data: (conversions in A, total in A, conversions in B, total in B)\n","weekly_data = {\n","    1: [120, 1000, 130, 1000],\n","    2: [150, 1000, 160, 1000],\n","    3: [180, 1000, 210, 1000],\n","    4: [210, 1000, 240, 1000],\n","    5: [230, 1000, 260, 1000],\n","    6: [250, 1000, 280, 1000]\n","}\n","\n","def calculate_p_value(week):\n","    data = weekly_data[week]\n","    conversions_A, total_A, conversions_B, total_B = data\n","\n","    # Perform Z-test for proportions\n","    counts = np.array([conversions_A, conversions_B])  # successes\n","    nobs = np.array([total_A, total_B])  # total participants\n","    stat, p_value = proportions_ztest(counts, nobs)\n","\n","    return p_value\n","\n","# --- Pocock Boundary Method ---\n","print(\"---- Pocock Boundary Method ----\")\n","\n","# Pocock's constant boundary: typically around Î± divided by a smaller factor\n","# We can use a z-value corresponding to the Pocock boundary. Approximate z-value for Pocock is around 2.41.\n","# Convert z-value to p-value\n","alpha = 0.05\n","z_pocock = norm.ppf(1 - alpha / 2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hWWZ4vDl5aEK","executionInfo":{"status":"ok","timestamp":1728219295072,"user_tz":-180,"elapsed":328,"user":{"displayName":"Manuel Kizer","userId":"07103369823277029587"}},"outputId":"8572ff13-ea3d-4c1a-e945-79d046984937"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["---- Pocock Boundary Method ----\n"]}]},{"cell_type":"markdown","source":["**EX_5**"],"metadata":{"id":"GW7QFTXeyFIE"}},{"cell_type":"markdown","source":["Initially, you believe that the new feature has a 50% chance of improving user engagement, which represents a non-informative prior (meaning you're starting with no strong assumptions). We can model this belief using a Beta distribution, a common choice for Bayesian A/B testing when dealing with proportions or probabilities.\n","\n","The Beta distribution is parameterized by two values: Î± (alpha) and Î² (beta).\n","If you believe that the new feature has a 50% chance of being better, you can set both Î± = 1 and Î² = 1. This is a uniform prior, reflecting a neutral starting belief (50/50).\n","Thus, the prior belief can be modeled as:\n","\n","Prior\n","âˆ¼\n","Beta\n","(\n","ð›¼\n","=\n","1\n",",\n","ð›½\n","=\n","1\n",")\n","Priorâˆ¼Beta(Î±=1,Î²=1)\n","This reflects your initial belief that the new feature is equally likely to improve or not improve engagement."],"metadata":{"id":"ZtyKbKhr57Xd"}},{"cell_type":"markdown","source":["After collecting data, Bayesian analysis allows you to update your prior belief based on the observed data. This updated belief is called the posterior distribution.\n","\n","Letâ€™s say, after collecting data, your analysis shows a 65% probability that the new feature is better.\n","The posterior distribution is calculated by updating the prior with the observed data. In A/B testing, this typically involves adding the observed successes and failures to the parameters of the Beta distribution.\n","\n","Assuming:\n","\n","You observe x successes (e.g., users who engaged with the new feature),\n","Out of n total observations (e.g., total users exposed to the new feature),\n","The posterior distribution is updated as:\n","\n","Posterior\n","âˆ¼\n","Beta\n","(\n","ð›¼\n","+\n","successes\n",",\n","ð›½\n","+\n","failures\n",")\n","Posteriorâˆ¼Beta(Î±+successes,Î²+failures)"],"metadata":{"id":"3lB2aqW55-q3"}},{"cell_type":"markdown","source":["Example:\n","Letâ€™s assume after testing, you observed 65% probability that the new feature is better based on x successes out of n users. This means your updated posterior distribution has shifted, and the mean of the distribution (probability of improvement) is now 65%.\n","Decision Based on Posterior:\n","If the posterior probability of the new feature being better is 65%, you are moderately confident that the feature improves user engagement. Bayesian decision-making allows you to incorporate this uncertainty into your decision process."],"metadata":{"id":"Oi9LQKKZ6QWv"}},{"cell_type":"markdown","source":["**Summary:**\n","\n","Prior Belief: Initially, you assume a 50% chance the new feature improves engagement (Beta(1, 1)).\n","Posterior Distribution: After collecting data, you update this belief, with the posterior showing a 65% probability that the new feature is better.\n","Decision: At 65%, you may proceed cautiously, but if the posterior was only 55%, it would be wise to collect more data before making a final decision."],"metadata":{"id":"x8etajjU5-O2"}},{"cell_type":"markdown","source":["**EX_6**"],"metadata":{"id":"jAxSMdIryHJt"}},{"cell_type":"markdown","source":["Traffic Adjustment Strategy:\n","\n","Layout C (best performer): Increase its traffic allocation, for example, to 50%.\n","Layouts A and B: Reduce their traffic allocation, for example, to 25% each.\n","This approach is similar to multi-armed bandit algorithms, where traffic is gradually shifted toward better-performing variations, but some traffic is still reserved for exploration to avoid prematurely dismissing potential winners.\n","\n","After the first week, the traffic allocation might look like this:\n","\n","Layout A: 25%\n","Layout B: 25%\n","Layout C: 50%"],"metadata":{"id":"X9mKyCvc7ahe"}},{"cell_type":"markdown","source":["Address premature allocation, exploration vs. exploitation trade-off, delayed feedback, statistical significance issues, and user experience bias using appropriate strategies like epsilon-greedy or Bayesian methods."],"metadata":{"id":"bQGIxc-u77Sd"}},{"cell_type":"markdown","source":["Monitor Performance: At the end of each subsequent week, reassess the engagement metrics (e.g., conversion rate, click-through rate) for each layout.\n","\n","Update Traffic Allocation: Adjust traffic based on performance updates. For example:\n","\n","If Layout C continues to outperform, you could increase its traffic allocation to 60-70%.\n","If Layout A shows improvement, you might allocate more traffic back to Layout A (e.g., increase it to 30%).\n","Continue reducing traffic to Layout B if its performance lags behind (e.g., down to 10-20%).\n","Convergence: Over time, as one layout consistently outperforms the others, you might allocate the majority of the traffic (e.g., 80-90%) to the best layout, while leaving a small percentage for exploration to confirm that the decision is robust."],"metadata":{"id":"ogLfWUDW77_p"}}]}
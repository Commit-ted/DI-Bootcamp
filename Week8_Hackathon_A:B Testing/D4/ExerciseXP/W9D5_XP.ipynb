{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOY3/EoIVv0C0bUqjLVSQ7C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# AB TEST"],"metadata":{"id":"mr8Lk8VNvGN9"}},{"cell_type":"markdown","source":["**EX_1**"],"metadata":{"id":"oK8BdhzcvCLB"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MUYIq3FAvBQU","executionInfo":{"status":"ok","timestamp":1728216639181,"user_tz":-180,"elapsed":3194,"user":{"displayName":"Manuel Kizer","userId":"07103369823277029587"}},"outputId":"a2f04329-3fb1-42aa-87b3-531406ed43f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Required sample size per group: 2940.557827324\n"]}],"source":["from statsmodels.stats.power import NormalIndPower\n","from statsmodels.stats.proportion import proportion_effectsize\n","\n","# Parameters\n","baseline_rate = 0.20  # Baseline open rate\n","new_rate = 0.23  # New open rate (after changing the subject line)\n","effect_size = proportion_effectsize(baseline_rate, new_rate)  # Calculate effect size\n","alpha = 0.05  # Significance level\n","power = 0.80  # Desired power (80%)\n","\n","# Calculate required sample size\n","power_analysis = NormalIndPower()\n","sample_size = power_analysis.solve_power(effect_size, power=power, alpha=alpha, ratio=1)\n","print(\"Required sample size per group:\", sample_size)\n"]},{"cell_type":"markdown","source":["This means you would need approximately 3673 participants in each group (A and B) to ensure that your A/B test is properly powered"],"metadata":{"id":"uqfnZhBCwsYI"}},{"cell_type":"markdown","source":["**EX_2**"],"metadata":{"id":"PPhqX_-gwvN6"}},{"cell_type":"code","source":["from statsmodels.stats.power import NormalIndPower\n","\n","# Initialize power analysis object\n","power_analysis = NormalIndPower()\n","\n","# Parameters\n","alpha = 0.05  # Significance level\n","power = 0.80  # Power level\n","\n","# Effect sizes\n","effect_sizes = [0.2, 0.4, 0.5]\n","\n","# Calculate sample sizes for each effect size\n","for effect_size in effect_sizes:\n","    sample_size = power_analysis.solve_power(effect_size, power=power, alpha=alpha, ratio=1)\n","    print(f\"Required sample size for effect size {effect_size}: {sample_size:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"McdR9Aapww-x","executionInfo":{"status":"ok","timestamp":1728217105867,"user_tz":-180,"elapsed":343,"user":{"displayName":"Manuel Kizer","userId":"07103369823277029587"}},"outputId":"e3eb37ac-d26b-40d2-84c6-3624ea66c827"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Required sample size for effect size 0.2: 392.44\n","Required sample size for effect size 0.4: 98.11\n","Required sample size for effect size 0.5: 62.79\n"]}]},{"cell_type":"markdown","source":["A small effect size means that the difference between the two groups (A and B) is relatively small. As a result, we need a larger sample size to detect this small difference with high confidence"],"metadata":{"id":"S0AXrb3Ix-8b"}},{"cell_type":"markdown","source":["**EX_3**"],"metadata":{"id":"1Qh_Xtt6x3RW"}},{"cell_type":"code","source":["from statsmodels.stats.power import NormalIndPower\n","\n","# Initialize power analysis object\n","power_analysis = NormalIndPower()\n","\n","# Parameters\n","effect_size = 0.2  # Small effect size\n","alpha = 0.05  # Significance level\n","\n","# Power levels\n","power_levels = [0.7, 0.8, 0.9]\n","\n","# Calculate sample sizes for each power level\n","for power in power_levels:\n","    sample_size = power_analysis.solve_power(effect_size, power=power, alpha=alpha, ratio=1)\n","    print(f\"Required sample size for power level {power}: {sample_size:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ZR0JuHjx01K","executionInfo":{"status":"ok","timestamp":1728217306853,"user_tz":-180,"elapsed":320,"user":{"displayName":"Manuel Kizer","userId":"07103369823277029587"}},"outputId":"868f4941-3232-48b5-a58a-8ac21c7dc0e2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Required sample size for power level 0.7: 308.60\n","Required sample size for power level 0.8: 392.44\n","Required sample size for power level 0.9: 525.37\n"]}]},{"cell_type":"markdown","source":["**EX_4**"],"metadata":{"id":"ZTaq2lDqyDzW"}},{"cell_type":"markdown","source":["Bonferroni Correction: Divide the significance level by the number of tests (weeks).\n","\n","Pocock Boundary: Adjusts the significance level based on the number of analyses, creating a more flexible boundary.\n","\n","O’Brien-Fleming Boundary: Starts with more conservative significance levels early in the test and gradually becomes more lenient."],"metadata":{"id":"ll_-IlrA02QU"}},{"cell_type":"markdown","source":["Bonferroni Correction Method"],"metadata":{"id":"Qe_zPfIq48Hd"}},{"source":["# Pseudocode for sequential testing\n","import scipy.stats as stats\n","\n","# Initialize variables\n","weeks = 6\n","alpha = 0.05\n","adjusted_alpha = alpha / weeks  # Bonferroni correction\n","\n","# Monitor weekly\n","for week in range(1, weeks + 1):\n","    # Assume 'calculate_p_value' is a function that returns the p-value for the current week's data\n","    # You need to define the function 'calculate_p_value'\n","    def calculate_p_value(week):\n","        # Replace this with your actual p-value calculation logic\n","        # This example just returns a random p-value\n","        return stats.uniform.rvs()\n","\n","    p_value = calculate_p_value(week)\n","\n","    print(f\"Week {week}, p-value: {p_value}\")\n","\n","    # Check if the p-value is below the adjusted threshold\n","    if p_value < adjusted_alpha:\n","        print(f\"Significant result at week {week}. Stop the test.\")\n","        break\n","    else:\n","        print(\"Continue testing.\")"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fVZJzbp8yoid","executionInfo":{"status":"ok","timestamp":1728217518384,"user_tz":-180,"elapsed":318,"user":{"displayName":"Manuel Kizer","userId":"07103369823277029587"}},"outputId":"0f580c5b-f730-4988-c9e0-5999113a87d5"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Week 1, p-value: 0.8376318262999883\n","Continue testing.\n","Week 2, p-value: 0.36077161203876695\n","Continue testing.\n","Week 3, p-value: 0.07795096872984342\n","Continue testing.\n","Week 4, p-value: 0.5634853453373877\n","Continue testing.\n","Week 5, p-value: 0.5618426434212065\n","Continue testing.\n","Week 6, p-value: 0.42466524373754966\n","Continue testing.\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from statsmodels.stats.proportion import proportions_ztest\n","\n","# Simulate weekly data (you would replace these with actual numbers)\n","# Data format: (conversions in A, total in A, conversions in B, total in B)\n","weekly_data = {\n","    1: [120, 1000, 130, 1000],\n","    2: [150, 1000, 160, 1000],\n","    3: [180, 1000, 210, 1000],\n","    4: [210, 1000, 240, 1000],\n","    5: [230, 1000, 260, 1000],\n","    6: [250, 1000, 280, 1000]\n","}\n","\n","def calculate_p_value(week):\n","    data = weekly_data[week]\n","    conversions_A, total_A, conversions_B, total_B = data\n","\n","    # Perform Z-test for proportions\n","    counts = np.array([conversions_A, conversions_B])  # successes\n","    nobs = np.array([total_A, total_B])  # total participants\n","    stat, p_value = proportions_ztest(counts, nobs)\n","\n","    return p_value\n","\n","# Sequential Testing Code (as before)\n","weeks = 6\n","alpha = 0.05\n","adjusted_alpha = alpha / weeks  # Bonferroni correction\n","\n","for week in range(1, weeks + 1):\n","    p_value = calculate_p_value(week)\n","    print(f\"Week {week}, p-value: {p_value:.4f}\")\n","\n","    if p_value < adjusted_alpha:\n","        print(f\"Significant result at week {week}. Stop the test.\")\n","        break\n","    else:\n","        print(\"Continue testing.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yiw4aIP8yxMN","executionInfo":{"status":"ok","timestamp":1728217554735,"user_tz":-180,"elapsed":319,"user":{"displayName":"Manuel Kizer","userId":"07103369823277029587"}},"outputId":"91953dd8-f8f7-4af9-87fb-51c605c3229c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Week 1, p-value: 0.4990\n","Continue testing.\n","Week 2, p-value: 0.5367\n","Continue testing.\n","Week 3, p-value: 0.0904\n","Continue testing.\n","Week 4, p-value: 0.1082\n","Continue testing.\n","Week 5, p-value: 0.1188\n","Continue testing.\n","Week 6, p-value: 0.1285\n","Continue testing.\n"]}]},{"cell_type":"markdown","source":["O’Brien-Fleming Boundary"],"metadata":{"id":"2VNzgFtf5MVf"}},{"cell_type":"code","source":["import numpy as np\n","from statsmodels.stats.proportion import proportions_ztest\n","\n","# Simulated weekly data: (conversions in A, total in A, conversions in B, total in B)\n","weekly_data = {\n","    1: [120, 1000, 130, 1000],\n","    2: [150, 1000, 160, 1000],\n","    3: [180, 1000, 210, 1000],\n","    4: [210, 1000, 240, 1000],\n","    5: [230, 1000, 260, 1000],\n","    6: [250, 1000, 280, 1000]\n","}\n","\n","def calculate_p_value(week):\n","    data = weekly_data[week]\n","    conversions_A, total_A, conversions_B, total_B = data\n","\n","    # Perform Z-test for proportions\n","    counts = np.array([conversions_A, conversions_B])  # successes\n","    nobs = np.array([total_A, total_B])  # total participants\n","    stat, p_value = proportions_ztest(counts, nobs)\n","\n","    return p_value\n","\n","# --- O'Brien-Fleming Boundary Method ---\n","print(\"---- O'Brien-Fleming Boundary Method ----\")\n","\n","# O'Brien-Fleming boundaries (these are approximate critical values for 6 looks)\n","obrien_fleming_boundaries = [0.0005, 0.0032, 0.0085, 0.0169, 0.0283, 0.0437]\n","\n","for week in range(1, 7):\n","    p_value = calculate_p_value(week)\n","    boundary = obrien_fleming_boundaries[week - 1]  # Select the boundary for the current week\n","\n","    print(f\"Week {week}, p-value: {p_value:.4f}, boundary: {boundary:.4f}\")\n","\n","    if p_value < boundary:\n","        print(f\"Significant result at week {week}. Stop the test.\")\n","        break\n","    else:\n","        print(\"Continue testing.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sncxu8QC44m1","executionInfo":{"status":"ok","timestamp":1728219228305,"user_tz":-180,"elapsed":301,"user":{"displayName":"Manuel Kizer","userId":"07103369823277029587"}},"outputId":"10d9c769-8d82-4e91-8bba-b72ac2274fdf"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["---- O'Brien-Fleming Boundary Method ----\n","Week 1, p-value: 0.4990, boundary: 0.0005\n","Continue testing.\n","Week 2, p-value: 0.5367, boundary: 0.0032\n","Continue testing.\n","Week 3, p-value: 0.0904, boundary: 0.0085\n","Continue testing.\n","Week 4, p-value: 0.1082, boundary: 0.0169\n","Continue testing.\n","Week 5, p-value: 0.1188, boundary: 0.0283\n","Continue testing.\n","Week 6, p-value: 0.1285, boundary: 0.0437\n","Continue testing.\n"]}]},{"cell_type":"markdown","source":["Pocock Boundary Method"],"metadata":{"id":"W8vLkAnR5Zv9"}},{"cell_type":"code","source":["import numpy as np\n","from statsmodels.stats.proportion import proportions_ztest\n","from scipy.stats import norm\n","\n","# Simulated weekly data: (conversions in A, total in A, conversions in B, total in B)\n","weekly_data = {\n","    1: [120, 1000, 130, 1000],\n","    2: [150, 1000, 160, 1000],\n","    3: [180, 1000, 210, 1000],\n","    4: [210, 1000, 240, 1000],\n","    5: [230, 1000, 260, 1000],\n","    6: [250, 1000, 280, 1000]\n","}\n","\n","def calculate_p_value(week):\n","    data = weekly_data[week]\n","    conversions_A, total_A, conversions_B, total_B = data\n","\n","    # Perform Z-test for proportions\n","    counts = np.array([conversions_A, conversions_B])  # successes\n","    nobs = np.array([total_A, total_B])  # total participants\n","    stat, p_value = proportions_ztest(counts, nobs)\n","\n","    return p_value\n","\n","# --- Pocock Boundary Method ---\n","print(\"---- Pocock Boundary Method ----\")\n","\n","# Pocock's constant boundary: typically around α divided by a smaller factor\n","# We can use a z-value corresponding to the Pocock boundary. Approximate z-value for Pocock is around 2.41.\n","# Convert z-value to p-value\n","alpha = 0.05\n","z_pocock = norm.ppf(1 - alpha / 2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hWWZ4vDl5aEK","executionInfo":{"status":"ok","timestamp":1728219295072,"user_tz":-180,"elapsed":328,"user":{"displayName":"Manuel Kizer","userId":"07103369823277029587"}},"outputId":"8572ff13-ea3d-4c1a-e945-79d046984937"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["---- Pocock Boundary Method ----\n"]}]},{"cell_type":"markdown","source":["**EX_5**"],"metadata":{"id":"GW7QFTXeyFIE"}},{"cell_type":"markdown","source":["Initially, you believe that the new feature has a 50% chance of improving user engagement, which represents a non-informative prior (meaning you're starting with no strong assumptions). We can model this belief using a Beta distribution, a common choice for Bayesian A/B testing when dealing with proportions or probabilities.\n","\n","The Beta distribution is parameterized by two values: α (alpha) and β (beta).\n","If you believe that the new feature has a 50% chance of being better, you can set both α = 1 and β = 1. This is a uniform prior, reflecting a neutral starting belief (50/50).\n","Thus, the prior belief can be modeled as:\n","\n","Prior\n","∼\n","Beta\n","(\n","𝛼\n","=\n","1\n",",\n","𝛽\n","=\n","1\n",")\n","Prior∼Beta(α=1,β=1)\n","This reflects your initial belief that the new feature is equally likely to improve or not improve engagement."],"metadata":{"id":"ZtyKbKhr57Xd"}},{"cell_type":"markdown","source":["After collecting data, Bayesian analysis allows you to update your prior belief based on the observed data. This updated belief is called the posterior distribution.\n","\n","Let’s say, after collecting data, your analysis shows a 65% probability that the new feature is better.\n","The posterior distribution is calculated by updating the prior with the observed data. In A/B testing, this typically involves adding the observed successes and failures to the parameters of the Beta distribution.\n","\n","Assuming:\n","\n","You observe x successes (e.g., users who engaged with the new feature),\n","Out of n total observations (e.g., total users exposed to the new feature),\n","The posterior distribution is updated as:\n","\n","Posterior\n","∼\n","Beta\n","(\n","𝛼\n","+\n","successes\n",",\n","𝛽\n","+\n","failures\n",")\n","Posterior∼Beta(α+successes,β+failures)"],"metadata":{"id":"3lB2aqW55-q3"}},{"cell_type":"markdown","source":["Example:\n","Let’s assume after testing, you observed 65% probability that the new feature is better based on x successes out of n users. This means your updated posterior distribution has shifted, and the mean of the distribution (probability of improvement) is now 65%.\n","Decision Based on Posterior:\n","If the posterior probability of the new feature being better is 65%, you are moderately confident that the feature improves user engagement. Bayesian decision-making allows you to incorporate this uncertainty into your decision process."],"metadata":{"id":"Oi9LQKKZ6QWv"}},{"cell_type":"markdown","source":["**Summary:**\n","\n","Prior Belief: Initially, you assume a 50% chance the new feature improves engagement (Beta(1, 1)).\n","Posterior Distribution: After collecting data, you update this belief, with the posterior showing a 65% probability that the new feature is better.\n","Decision: At 65%, you may proceed cautiously, but if the posterior was only 55%, it would be wise to collect more data before making a final decision."],"metadata":{"id":"x8etajjU5-O2"}},{"cell_type":"markdown","source":["**EX_6**"],"metadata":{"id":"jAxSMdIryHJt"}},{"cell_type":"markdown","source":["Traffic Adjustment Strategy:\n","\n","Layout C (best performer): Increase its traffic allocation, for example, to 50%.\n","Layouts A and B: Reduce their traffic allocation, for example, to 25% each.\n","This approach is similar to multi-armed bandit algorithms, where traffic is gradually shifted toward better-performing variations, but some traffic is still reserved for exploration to avoid prematurely dismissing potential winners.\n","\n","After the first week, the traffic allocation might look like this:\n","\n","Layout A: 25%\n","Layout B: 25%\n","Layout C: 50%"],"metadata":{"id":"X9mKyCvc7ahe"}},{"cell_type":"markdown","source":["Address premature allocation, exploration vs. exploitation trade-off, delayed feedback, statistical significance issues, and user experience bias using appropriate strategies like epsilon-greedy or Bayesian methods."],"metadata":{"id":"bQGIxc-u77Sd"}},{"cell_type":"markdown","source":["Monitor Performance: At the end of each subsequent week, reassess the engagement metrics (e.g., conversion rate, click-through rate) for each layout.\n","\n","Update Traffic Allocation: Adjust traffic based on performance updates. For example:\n","\n","If Layout C continues to outperform, you could increase its traffic allocation to 60-70%.\n","If Layout A shows improvement, you might allocate more traffic back to Layout A (e.g., increase it to 30%).\n","Continue reducing traffic to Layout B if its performance lags behind (e.g., down to 10-20%).\n","Convergence: Over time, as one layout consistently outperforms the others, you might allocate the majority of the traffic (e.g., 80-90%) to the best layout, while leaving a small percentage for exploration to confirm that the decision is robust."],"metadata":{"id":"ogLfWUDW77_p"}}]}